= Compiling and Installing Cbench =

== Compiling and Installing the core of Cbench ==

Cbench requires the following environment variables be set:

 * CBENCHOME - Location of the Cbench distribution tree
 * MPIHOME   - Location of the MPI tree where we look to find MPI compile scripts (i.e. mpicc, mpif77) and MPI includes
 * COMPILERCOLLECTION - Pick the compiler collection/chain you want to compile with. Examples include: intel, gcc, pgi.

NOTE: make.def must support the specified compiler collection on the machine architecture you are using.
						 
The key file in Cbench for controlling compilation is the make.def file. To
really control what happens during compilation you'll need to take a look at
make.def and possibly edit it.  You are often able to use support that is
already included in make.def.  A key variable to look at is BLASLIB since this
must be set correclty for HPLinpack (xhpl) and other utilities that link with
external BLAS libraries to compile correctly.

There is a  sample Cbench module file you can place in your modulefiles directory.
There is also a simple bash and tcsh style script here as well.

Once you have your environment setup and make.def setup, the cluster config file,
cluster.def, for the cluster you want to test needs to be edited.  All the
parameters in cluster.def should be commented.

Then make and install the standard collection of tests:
 * make install

This compiles and installs the standard collection of test binaries in $CBENCHOME/bin.

== Installing Supported Cbench Test Sets ==

Compile Cbench as described above.  Install the Cbench test set tree:
 * make installtests

This will install all test sets that are currently packaged in Cbench into a tree named $CBENCHOME-test by default. An alternate tree location can be specified by setting the CBENCHTEST environment variable:
 * e.g. export CBENCHTEST=/scratch3/user/cbench-test

== Compiling non-default parts of Cbench ==
Some code within Cbench is not compiled by default.  HPC Challenge and NAS Parallel Benchmarks are two primary ones.  HPCC eventually will compile by default after the Cbench build system is upgraded to something a little smarter.  This is because HPCC can be built two ways in Cbench:
 1. with the normal MPI libraries and compilers as specified by MPIHOME
 2. with a build of MPICH self-contained within Cbench which is useful for building MPI tests for use in the ''nodehwtest'' testset.

The NAS Parallel Benchmarks has to build a binary for every single combination of processor count, test (sp, ft,...), and test size (A,B,C,...) so we don't build them by default.  This can be a lot of binaries depending on the size of your cluster as setup in .[source:trunk/cbench/cluster.def cluster.def].

=== Building HPCC with the system MPI ===
From the top-level of the Cbench source tree:
 * make -C opensource/hpcc distclean
 * make -C opensource/hpcc
 * make -C opensource/hpcc install
 * make itests   (to update the CBENCTEST tree)

=== Building NPB ===
From the top-level of the Cbench source tree:
 * make -C opensource/NPB
OR to limit the size of the build even further than what is in cluster.def (i.e. your cluster has 4096 nodes but you only want to be able run NPB jobs up to 512 nodes)
 * make MAXPROCS=512 -C opensource/NPB

To make sure your NPB binaries get updated in the CBENCHTEST tree, from the top-level of the Cbench tree run :
 * sbin/install_npb

=== Building the self-contained MPICH for Cbench ===
 * make -C opensource/mpich

=== Building HPCC with the Cbench MPICH ===
 * make -C opensource/hpcc distclean
 * make -C opensource/hpcc local
 * make -C opensource/hpcc install
 * make itests   (to update the CBENCTEST tree)
